{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "19cIz8x8_iMdkOa7yyK980jxFh1vNZ7_T",
      "authorship_tag": "ABX9TyMRmILgWszKejURD9ny0t9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waitkeeper/aistudy/blob/main/LLMs_from_scratch_CN-02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学习教程\n",
        "\n",
        "B站视频\n",
        "https://www.bilibili.com/video/BV16AKAzzECq/?spm_id_from=333.1387.favlist.content.click&vd_source=795bf9ea159d202907a8da08d96e68b8\n",
        "\n",
        "中文版文档资料\n",
        "https://skindhu.github.io/Build-A-Large-Language-Model-CN/#/./cn-Book/1.%E7%90%86%E8%A7%A3%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B\n"
      ],
      "metadata": {
        "id": "K7IXVeojgGjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "otkX7NV_bA6q",
        "outputId": "ce1d44f9-c0d9-4795-adc4-fc78cdc3ca50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install torch\n",
        "!pip install tiktoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 数据准备\n"
      ],
      "metadata": {
        "id": "ivMTzyuoeXz5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dee93255",
        "outputId": "af9190bb-5760-448e-af0e-5b396fd1c3a1"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "filename = \"the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"File '{filename}' downloaded successfully.\")\n",
        "else:\n",
        "    print(f\"File '{filename}' already exists.\")\n",
        "\n",
        "# Listing 2.1 Reading in a short story as text sample into Python\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'the-verdict.txt' downloaded successfully.\n",
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 文本分词"
      ],
      "metadata": {
        "id": "lcEfrf2seUkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 自测试数据\n",
        "import re\n",
        "text_ori = \"Hello, world. This, is a test.\"\n",
        "result01 = re.split(r'(\\s)', text_ori)\n",
        "print(result01)\n",
        "result02 = re.split(r'([,.]|\\s)', text_ori)\n",
        "print(result02)\n",
        "result02_1 = [item for item in result02 if item.strip()]\n",
        "print(result02_1)\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH1nv4jxcWZ6",
        "outputId": "41812cf7-9393-4eca-9e7c-8939e6725730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n",
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCaI6wLTdVRC",
        "outputId": "28fb10ff-8ca3-467b-f216-a8ba18766255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kvv4-gNkeLqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 将tokens转化为token ID"
      ],
      "metadata": {
        "id": "sNniPPnfe-fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set的作用是去除重复元素\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)\n",
        "# 这个语法是python中的字典推导式\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i > 10:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KlMR_2RfGHk",
        "outputId": "3e7d1628-2362-477c-8db9-95d7b5a3792c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# token到 tokenID，tokenID也需要转为token\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {integer: token for token, integer in vocab.items()}\n",
        "    def encode(self,text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[str] for str in preprocessed]\n",
        "        return ids\n",
        "    def decode(self,ids):\n",
        "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "oHZoOd6WidC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试上面的类\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "tokens = tokenizer.decode(ids)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjXUYhxHlHDP",
        "outputId": "fee8dedb-29f9-487c-f37a-4e8c873a7ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 问题\n",
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))\n",
        "# 这个报错的原因是我们的文章中没有Hello这个单词"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "k9IIAnwPlKqT",
        "outputId": "97fa9425-2ff7-435b-a854-4100613ceb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-70990378.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 问题\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# 这个报错的原因是我们的文章中没有Hello这个单词\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3933622116.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allTokens = sorted(list(set(preprocessed)))\n",
        "allTokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer,token in enumerate(allTokens)}\n",
        "print(len(vocab.items()))"
      ],
      "metadata": {
        "id": "NZ0kQOb_niOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 开始编写第二版的\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self,vocab) -> None:\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "    def encode(self,text):\n",
        "        preTokens = re.split(r'([,.?_!\"()\\']|--|\\s)',text)\n",
        "        preTokens = [item.strip() for item in preTokens if item.strip()]\n",
        "        preTokens = [item if item in self.str_to_int else \"<|unk|>\" for item in preTokens]\n",
        "        ids = [self.str_to_int[s] for s in preTokens ]\n",
        "        return ids\n",
        "    def decode(self,ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "l0YcrKsu_wpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "V7HOt9A0AsOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizerV2 = SimpleTokenizerV2(vocab)\n",
        "resIds = tokenizerV2.encode(text)\n",
        "resText = tokenizerV2.decode(resIds)\n",
        "print(resText)\n"
      ],
      "metadata": {
        "id": "TOpeli6sIsMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cae5fj8eJDio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 字节对编码"
      ],
      "metadata": {
        "id": "qGHwNePhJdmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tiktoken\n",
        "\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "\n",
        "print(\"toktoken version：\",version(\"tiktoken\"))\n",
        "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
        "tokenizer01 = tiktoken.get_encoding('gpt2')\n",
        "integer01 = tokenizer01.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(integer01) # 这里就会发现 <|endoftext|> 被分配了一个很大的id，50256，这是gpt2的最大tokenid\n",
        "recoverText = tokenizer01.decode(integer01)\n",
        "print(recoverText)"
      ],
      "metadata": {
        "id": "2IEF6VIxJiK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用滑动窗口来进行数据采样"
      ],
      "metadata": {
        "id": "nx90cT27Ou0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('the-verdict.txt','r',encoding='utf-8')as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer01.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "id": "NMQ4NzOIOzTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 # 上下文大小决定了输入中有多少个token\n",
        "\n",
        "enc_sample = enc_text[:50]\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x:{x}\")\n",
        "print(f\"y:    {y}\")\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(context, \"--->\", desired)\n",
        "    print(tokenizer01.decode(context),\"--->\", tokenizer01.decode([desired]))"
      ],
      "metadata": {
        "id": "Uyzq_FsVQ2_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 实现数据加载器类\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import tiktoken\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self,txt,tokenizer,max_lenght,stride) -> None:\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "        token_ids = tokenizer.encode(txt)\n",
        "        # 使用滑动窗口将书籍分块为最大长度的重叠序列\n",
        "        for i in range(0,len(token_ids)-max_lenght,stride):\n",
        "            input_chunk = token_ids[i:i+max_lenght]\n",
        "            target_chunk = token_ids[i+1:i+max_lenght+1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, index):\n",
        "        return self.input_ids[index],self.target_ids[index]\n",
        "\n",
        "# 使用上面刚创建的 GPTDatasetV1类，通过 PyTorch DataLoader以批量方式加载输入\n",
        "def create_dataloader_v1(txt,batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True,drop_last=True,num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding('gpt2')\n",
        "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "McanfCqGRQSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试下上面的类\n",
        "with open('the-verdict.txt',\"r\",encoding='utf-8') as f :\n",
        "    raw_text = f.read()\n",
        "dataloader = create_dataloader_v1(raw_text,batch_size=1,max_length=4,stride=1,shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "id": "125GB3_wZZP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 上面我们的批次都是1，主要是为了说明运作原理。\n",
        "# 在深度学习中，小批次在训练时消耗的内存较小，但会使模型更新更加苦难"
      ],
      "metadata": {
        "id": "-iBpGfAEZ6H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 构建词嵌入层\n",
        "\n",
        "为LLM准备训练集的最后一步是将token ID转化为嵌入向量。\n",
        "我们首先会以随机值的方式初始化这些嵌入权重。 后面还会优化嵌入权重，作为LLM训练的一部分"
      ],
      "metadata": {
        "id": "hW1OKcpqbqLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 用一个例子来说明tokenId到嵌入向量转化的工作原理\n",
        "\n",
        "# 假设有 4 个 token\n",
        "input_ids = torch.tensor([2,3,5,1])\n",
        "# 假设我们词汇表只有6个单词\n",
        "vocab_size = 6\n",
        "# 假设嵌入向量的维度是3 （在GPT-3中，嵌入大小的维度是12288维）\n",
        "output_dim = 3\n",
        "\n",
        "# 使用 vocab_size 和 output_dim 在pytorch中实例化一个嵌入层\n",
        "# 随机种子 123，方便结果可复现\n",
        "torch.manual_seed(123)\n",
        "\n",
        "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
        "print(embedding_layer.weight)\n",
        "# 嵌入矩阵的每一行表示词汇表中的一个token，每个token都有唯一的向量表示\n",
        "# 嵌入矩阵中的每一列表示嵌入空间中的一个维度，当前例子表示嵌入空间有3个维度"
      ],
      "metadata": {
        "id": "eXVElU7BcRZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 有了嵌入层后，我们就可以通过它获取指定tokenId的嵌入向量\n",
        "print(embedding_layer(torch.tensor([3])))\n",
        "# 在 PyTorch 中，当你像函数一样调用一个 nn.Module 的实例（例如 embedding_layer(input_tensor)）时\n",
        "# 实际上是调用了该模块的 forward 方法。\n",
        "# 对于 nn.Embedding 模块，它的 forward 方法会接收一个包含 token ID 的张量作为输入，\n",
        "# 然后查找这些 token ID 对应的嵌入向量（embedding vectors），并返回这些向量组成的张量。"
      ],
      "metadata": {
        "id": "yAI5nlSlfx5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4个输入token id的情况\n",
        "print(embedding_layer(torch.tensor([2,3,5,1])))"
      ],
      "metadata": {
        "id": "D4gNDb1kgbyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 位置编码\n",
        "\n",
        "上一节中，我们将token id转换为连续的向量表示，就是token嵌入\n",
        "\n",
        "从格式上来说，这是适合作为LLM的输入的，然后LLM的一个小缺点是他的自注意力机制对序列中的token的位置或者顺序没有概念\n",
        "\n",
        "而我们前面嵌入层的引入方式是，无论token处于序列中的什么位置，最后得到的嵌入向量都是相同的\n",
        "\n",
        "怎么解决： 位置嵌入\n",
        "\n",
        "嵌入有两种\n",
        "1. 绝对位置嵌入\n",
        "2. 相对位置嵌入\n",
        "相对位置嵌入强调的是token之前的相对位置或者举例。\n",
        "\n"
      ],
      "metadata": {
        "id": "DHxq2QYshWrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 调整下嵌入层\n",
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "IEYkg8MhhbZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "      raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\", inputs)\n",
        "print(\"Inputs shape:\", inputs.shape)"
      ],
      "metadata": {
        "id": "XTStzGZijGco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用嵌入层将这些token id转化为256维度的向量\n",
        "token_embedding = token_embedding_layer(inputs)\n",
        "print(token_embedding.shape)"
      ],
      "metadata": {
        "id": "T9dPueY7jH25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 对于GPT模型使用位置绝对嵌入，我们只需要创建另一个嵌入层，维度和token_embedding_layer 一致\n",
        "\n",
        "context_length = max_length\n",
        "pos_embeding_layer = torch.nn.Embedding(context_length,output_dim)\n",
        "pos_embeding = pos_embeding_layer(torch.arange(context_length))\n",
        "print(pos_embeding.shape)\n"
      ],
      "metadata": {
        "id": "xYgG4AQtjoVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_embedding = token_embedding + pos_embeding\n",
        "print(input_embedding)"
      ],
      "metadata": {
        "id": "9KfbDZrUl1Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "znfCXdBJkxlZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}